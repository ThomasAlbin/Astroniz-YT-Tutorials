{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bcc812e",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/[ML1]-Asteroid-Spectra/6_ml_svm_search.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c9d7a",
   "metadata": {},
   "source": [
    "# Step 6: Machine Learning - Parameter Search / Optimization\n",
    "\n",
    "In this step we are performing a GridSearch on the binary classification problem. Our considered metric: the F1 score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Import installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mount the Google Drive, where we store files and models (if applicable, otherwise work\n",
    "# locally)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    core_path = \"/gdrive/MyDrive/Colab/asteroid_taxonomy/\"\n",
    "except ModuleNotFoundError:\n",
    "    core_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 2 asteroid data\n",
    "asteroids_df = pd.read_pickle(os.path.join(core_path, \"data/lvl2/\", \"asteroids.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f27a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add a binary classification schema, where we distinguish between e.g., X and non-X classes\n",
    "asteroids_df.loc[:, \"Class\"] = asteroids_df[\"Main_Group\"].apply(lambda x: 1 if x==\"X\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62493261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the spectra to one array and the classes to another one\n",
    "asteroids_X = np.array([k[\"Reflectance_norm550nm\"].tolist() for k in asteroids_df[\"SpectrumDF\"]])\n",
    "asteroids_y = np.array(asteroids_df[\"Class\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39ead307-7368-4991-925d-9daba407e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we create a single test-training split with a ratio of 0.8 / 0.2\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "\n",
    "# Create a simple, single train / test split\n",
    "for train_index, test_index in sss.split(asteroids_X, asteroids_y):\n",
    "    \n",
    "    X_train, X_test = asteroids_X[train_index], asteroids_X[test_index]\n",
    "    y_train, y_test = asteroids_y[train_index], asteroids_y[test_index]\n",
    "\n",
    "# Compute class weightning\n",
    "positive_class_weight = int(1.0 / (sum(y_train) / len(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87548b42-2b1f-4f2b-8718-600394d91b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV 1/5] END ................C=1, kernel=linear;, score=0.584 total time=   0.0s\n",
      "[CV 2/5] END ................C=1, kernel=linear;, score=0.508 total time=   0.0s\n",
      "[CV 3/5] END ................C=1, kernel=linear;, score=0.602 total time=   0.0s\n",
      "[CV 4/5] END ................C=1, kernel=linear;, score=0.562 total time=   0.0s\n",
      "[CV 5/5] END ................C=1, kernel=linear;, score=0.564 total time=   0.0s\n",
      "[CV 1/5] END ...............C=10, kernel=linear;, score=0.595 total time=   0.2s\n",
      "[CV 2/5] END ...............C=10, kernel=linear;, score=0.508 total time=   0.1s\n",
      "[CV 3/5] END ...............C=10, kernel=linear;, score=0.610 total time=   0.2s\n",
      "[CV 4/5] END ...............C=10, kernel=linear;, score=0.554 total time=   0.2s\n",
      "[CV 5/5] END ...............C=10, kernel=linear;, score=0.558 total time=   0.2s\n",
      "[CV 1/5] END ..............C=100, kernel=linear;, score=0.579 total time=   1.9s\n",
      "[CV 2/5] END ..............C=100, kernel=linear;, score=0.516 total time=   2.1s\n",
      "[CV 3/5] END ..............C=100, kernel=linear;, score=0.615 total time=   1.6s\n",
      "[CV 4/5] END ..............C=100, kernel=linear;, score=0.547 total time=   1.5s\n",
      "[CV 5/5] END ..............C=100, kernel=linear;, score=0.558 total time=   1.9s\n",
      "[CV 1/5] END ...................C=1, kernel=rbf;, score=0.874 total time=   0.0s\n",
      "[CV 2/5] END ...................C=1, kernel=rbf;, score=0.763 total time=   0.0s\n",
      "[CV 3/5] END ...................C=1, kernel=rbf;, score=0.791 total time=   0.0s\n",
      "[CV 4/5] END ...................C=1, kernel=rbf;, score=0.854 total time=   0.0s\n",
      "[CV 5/5] END ...................C=1, kernel=rbf;, score=0.889 total time=   0.0s\n",
      "[CV 1/5] END ..................C=10, kernel=rbf;, score=0.927 total time=   0.0s\n",
      "[CV 2/5] END ..................C=10, kernel=rbf;, score=0.851 total time=   0.0s\n",
      "[CV 3/5] END ..................C=10, kernel=rbf;, score=0.911 total time=   0.0s\n",
      "[CV 4/5] END ..................C=10, kernel=rbf;, score=0.894 total time=   0.0s\n",
      "[CV 5/5] END ..................C=10, kernel=rbf;, score=0.935 total time=   0.0s\n",
      "[CV 1/5] END .................C=100, kernel=rbf;, score=0.937 total time=   0.0s\n",
      "[CV 2/5] END .................C=100, kernel=rbf;, score=0.892 total time=   0.0s\n",
      "[CV 3/5] END .................C=100, kernel=rbf;, score=0.921 total time=   0.0s\n",
      "[CV 4/5] END .................C=100, kernel=rbf;, score=0.950 total time=   0.0s\n",
      "[CV 5/5] END .................C=100, kernel=rbf;, score=0.895 total time=   0.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(class_weight={1: 5}),\n",
       "             param_grid=[{'C': [1, 10, 100], 'kernel': ['linear']},\n",
       "                         {'C': [1, 10, 100], 'kernel': ['rbf']}],\n",
       "             scoring='f1', verbose=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform now a GridSearch with the following parameter range and kernels\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100], 'kernel': ['rbf']},\n",
    " ]\n",
    "\n",
    "# Set the SVM classifier\n",
    "svc = svm.SVC(class_weight={1: positive_class_weight})\n",
    "\n",
    "# Instantiate the StandardScaler (mean 0, standard deviation 1) and use the training data to fit\n",
    "# the scaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform now the training data\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "# Set the GridSearch and ...\n",
    "wclf = GridSearchCV(svc, param_grid, scoring='f1', verbose=3, cv=5)\n",
    "\n",
    "# ... perform the training!\n",
    "wclf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c0d08f-5dea-4595-9041-30201d1a91f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: get the best estimator\n",
    "final_clf = wclf.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6985e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing data ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... and perform a predicition\n",
    "y_test_pred = final_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97aa4039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[215   6]\n",
      " [  3  44]]\n"
     ]
    }
   ],
   "source": [
    "# Import the confusion matrix and perform the computation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# The order of the confusion matrix is:\n",
    "#     - true negative (top left, tn)\n",
    "#     - false positive (top right, fp)\n",
    "#     - false negative (bottom left, fn)\n",
    "#     - true positive (bottom right, tp)\n",
    "tn, fp, fn, tp = conf_mat.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5686e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.936\n",
      "Precision Score: 0.88\n",
      "F1 Score: 0.907\n"
     ]
    }
   ],
   "source": [
    "# Recall: ratio of correctly classified X Class spectra, considering the false negatives\n",
    "# (recall = tp / (tp + fn))\n",
    "recall_score = round(sklearn.metrics.recall_score(y_test, y_test_pred), 3)\n",
    "print(f\"Recall Score: {recall_score}\")\n",
    "\n",
    "# Precision: ratio of correctly classified X Class spectra, considering the false positives\n",
    "# (precision = tp / (tp + fp))\n",
    "precision_score = round(sklearn.metrics.precision_score(y_test, y_test_pred), 3)\n",
    "print(f\"Precision Score: {precision_score}\")\n",
    "\n",
    "# A combined score\n",
    "f1_score = round(sklearn.metrics.f1_score(y_test, y_test_pred), 3)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b2fe6-1b77-4af6-b0f0-3fbd2f444243",
   "metadata": {},
   "source": [
    "# Summary / Outlook:\n",
    "\n",
    "To finalize: Apply all data (take care of scaling!) on the training and rerun it. Save it afterwards for further computations. Storing and using a model requires also to store the corresponding scaler (store them as pickle files)!\n",
    "\n",
    "Further notes:<br>\n",
    "- If you would like to improve the model later on, one needs to perform \"partial fits\" to improve / train the weight(s). A simple \"fit\" re-runs the training and computes the model from scratch. Only linear SVMs can be partially fitted using [SGDClassifiers](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)\n",
    "- Using all data to train a final model requires a proper metric. Cross validation as shown above can be used, but one should consider \"Nested Cross Validation\" methods as shown [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "- Also: a computational more extensive method called [HalvingGridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV) may improve the model even further"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
