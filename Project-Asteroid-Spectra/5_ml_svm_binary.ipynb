{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1180265",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ThomasAlbin/Astroniz-YT-Tutorials/blob/main/[ML1]-Asteroid-Spectra/5_ml_svm_binary.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c9d7a",
   "metadata": {},
   "source": [
    "# Step 5: Machine Learning - Support Vector Machine (binary classes)\n",
    "\n",
    "We are now set to perform some Machine Learning on the Asteroid Spectra Data! We keep it simple though: The multiclass clssificaiton problem of the Main Group classes:\n",
    "\n",
    "- C\n",
    "- S\n",
    "- X\n",
    "- Other\n",
    "\n",
    "... shall be transformed into a binary problem. E.g.: X (1) and Not-X (0). In this script we use a Support Vector Machine (SVM) algorithm to perform some classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63474560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import os\n",
    "\n",
    "# Import installed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af9f8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's mount the Google Drive, where we store files and models (if applicable, otherwise work\n",
    "# locally)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    core_path = \"/gdrive/MyDrive/Colab/asteroid_taxonomy/\"\n",
    "except ModuleNotFoundError:\n",
    "    core_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bc69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the level 2 asteroid data\n",
    "asteroids_df = pd.read_pickle(os.path.join(core_path, \"data/lvl2/\", \"asteroids.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22932825-f38c-4c28-b562-e45fc4d29aac",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "First, let's create a binary classification of the \"X\" and \"Non-X\" class. We then convert the dataframe to numpy arrays that are being handled by scikit-learn later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36f27a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we add a binary classification schema, where we distinguish between e.g., X and non-X classes\n",
    "asteroids_df.loc[:, \"Class\"] = asteroids_df[\"Main_Group\"].apply(lambda x: 1 if x==\"X\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62493261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate the spectra to one array and the classes to another one\n",
    "asteroids_X = np.array([k[\"Reflectance_norm550nm\"].tolist() for k in asteroids_df[\"SpectrumDF\"]])\n",
    "asteroids_y = np.array(asteroids_df[\"Class\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4572a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we create a single test-training split with a ratio of 0.8 / 0.2\n",
    "# The StratifiedShuffleSplit is needed to preserve the ratio of the classes!\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
    "\n",
    "for train_index, test_index in sss.split(asteroids_X, asteroids_y):\n",
    "    X_train, X_test = asteroids_X[train_index], asteroids_X[test_index]\n",
    "    y_train, y_test = asteroids_y[train_index], asteroids_y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c26c24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive training classes: 0.18\n",
      "Ratio of positive test classes: 0.18\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look whether the unbalanced ratio has been preserved\n",
    "print(f\"Ratio of positive training classes: {round(sum(y_train) / len(X_train), 2)}\")\n",
    "print(f\"Ratio of positive test classes: {round(sum(y_test) / len(X_test), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd69d7fd",
   "metadata": {},
   "source": [
    "## Imbalanced Datasets\n",
    "The inverse of the ratio is approximately 5. We need this, to weight our imbalanced training set during the ML fitting process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47298888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Class weightning: 5\n"
     ]
    }
   ],
   "source": [
    "# Compute class weightning\n",
    "positive_class_weight = int(1.0 / (sum(y_train) / len(X_train)))\n",
    "print(f\"Positive Class weightning: {positive_class_weight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5b34f",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "\n",
    "Scaling the training and test data should be done AFTER the splitting to prevent data leakage from the test data to the training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd171dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the preprocessing module\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Instantiate the StandardScaler (mean 0, standard deviation 1) and use the training data to fit\n",
    "# the scaler\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "# Transform now the training data\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee32a9a",
   "metadata": {},
   "source": [
    "## The training\n",
    "\n",
    "Please note: This training is rather simple; it does not use multiple training / test splits, performs no logging and performs no GridSearch to find the best set of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66fa81c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, class_weight={1: 5})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the SVM class\n",
    "from sklearn import svm\n",
    "\n",
    "# Call the SVM class, use an RBF kernel and apply the class weightning. That's it!\n",
    "wclf = svm.SVC(kernel='rbf', class_weight={1: positive_class_weight}, C=100)\n",
    "\n",
    "# Perform the training\n",
    "wclf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6985e960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the testing data ...\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ... and perform a predicition\n",
    "y_test_pred = wclf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde19b54",
   "metadata": {},
   "source": [
    "## Metrics - a first look\n",
    "\n",
    "<i>What kind of metrics are useful to determine the performance of a classifier?</i>\n",
    "\n",
    "The answer to this questions depends on one's expectations on a classifiers behaviour: Is the \"overall\" performance important? Does one want to prevent false positives? Or false negatives? Is the number of positive results the most important metric?\n",
    "\n",
    "Fictional project scope: Let's assume that our final model shall later support scientists as a tool to find X Class spectra in a vast dataset. Further, let's assume that the scientists' task is to find as many X Class spectra as possible. Consequently the scientists do not want to have falsely classified X spectra. We take a look first at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97aa4039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[217   4]\n",
      " [  2  45]]\n"
     ]
    }
   ],
   "source": [
    "# Import the confusion matrix and perform the computation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(conf_mat)\n",
    "\n",
    "# The order of the confusion matrix is:\n",
    "#     - true negative (top left, tn)\n",
    "#     - false positive (top right, fp)\n",
    "#     - false negative (bottom left, fn)\n",
    "#     - true positive (bottom right, tp)\n",
    "tn, fp, fn, tp = conf_mat.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a40869",
   "metadata": {},
   "source": [
    "## Metrics - what matters\n",
    "\n",
    "Fictional project scope: The number of false negatives should be as small as possible (the scientist does not want to lose preciouse spectra!). Further, the number of false positives might be higher. The scientists will sort them out later on. A less strict requirement for the so called False Discovery Rate enables the Machine Learning Engineer to find a solution quicker in a broader solution space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5686e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall Score: 0.957\n",
      "Precision Score: 0.918\n",
      "F1 Score: 0.938\n"
     ]
    }
   ],
   "source": [
    "# Recall: ratio of correctly classified X Class spectra, considering the false negatives\n",
    "# (recall = tp / (tp + fn))\n",
    "recall_score = round(sklearn.metrics.recall_score(y_test, y_test_pred), 3)\n",
    "print(f\"Recall Score: {recall_score}\")\n",
    "\n",
    "# Precision: ratio of correctly classified X Class spectra, considering the false positives\n",
    "# (precision = tp / (tp + fp))\n",
    "precision_score = round(sklearn.metrics.precision_score(y_test, y_test_pred), 3)\n",
    "print(f\"Precision Score: {precision_score}\")\n",
    "\n",
    "# A combined score\n",
    "f1_score = round(sklearn.metrics.f1_score(y_test, y_test_pred), 3)\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55520882-cd78-4bde-9941-6ec863538bac",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Machine Learning models results do in fact not matter if one does not have a suitably defined baseline. What would a random naive classifier do? Well let's check is in the most simple way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d37396-5502-41f8-b556-1cb3c4a20ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We copy the original labelling and shuffle it randomly\n",
    "asteroids_random_y = asteroids_y.copy()\n",
    "np.random.shuffle(asteroids_random_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4b8d15c-9822-46d5-8b1e-949bf66b3eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive F1 Score: 0.131\n"
     ]
    }
   ],
   "source": [
    "# Now we can apply e.g., the F1 score on the random-classifier. Please note: theoretically this\n",
    "# should have been done before the training! But in this initial video we keep it simple in a more\n",
    "# \"storytelling\" way\n",
    "f1_score_naive = round(sklearn.metrics.f1_score(asteroids_y, asteroids_random_y), 3)\n",
    "print(f\"Naive F1 Score: {f1_score_naive}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
